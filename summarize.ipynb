{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import csv\n!pip install datasets\nfrom datasets import load_dataset\nimport pandas as pd\n!pip install sentencepiece\n!pip install datasets\n!pip install transformers\nfrom datasets import load_dataset\n!pip install torch\nimport torch as th\nfrom transformers import T5Tokenizer, T5Model,T5ForConditionalGeneration,T5Config\nfrom transformers import get_scheduler\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import DataLoader\nfrom transformers import AdamW\nimport torch\nfrom transformers import Trainer, TrainingArguments\nfrom datasets import load_dataset\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET LOADING","metadata":{}},{"cell_type":"code","source":"\ndataset_file = '/news_summary_more.csv'\n\ndataset = load_dataset('csv', data_files=dataset_file, split='train')\n\ndataset = dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset['train']\nval_dataset = dataset['test']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CREATING TOKENIZER AND MODEL","metadata":{}},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\nconfig = T5Config.from_pretrained(\"t5-small\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"t5-small\",config=config,)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINDING MAXIMUM LENGTH OF TEXTS AFTER TOKENIZATION","metadata":{}},{"cell_type":"code","source":"#finding maximum padding \nsumry = list(train_dataset['headlines'])\ntxt = list(train_dataset['text'])\nsumry_t = tokenizer(sumry,padding=False,truncation=False)\ntxt_t = tokenizer(txt,padding=False,truncation=False)\nmax_source=0\nfor item in sumry_t['input_ids']:\n    if len(item) > max_source:\n        max_source = len(item)\n\nmax_target = 0\nfor item in txt_t['input_ids']:\n    if len(item) > max_target:\n        max_target = len(item)\nmax_txt_pad=max_target\nmax_smry_pad=max_source\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATASET CREATION FOR TRAINING","metadata":{}},{"cell_type":"code","source":"\nclass GSMDataset(th.utils.data.Dataset):\n    def __init__(self,tokenizer,dataset,pad_text,pad_sum, loss_on_prefix=True):\n        self.examples = dataset\n        self.pad_text=pad_text\n        self.pad_sum=pad_sum\n        self.sumry = list(self.examples['headlines'])\n        self.txt = list(self.examples['text'])\n        self.txts = tokenizer(self.txt,padding='max_length', max_length=self.pad_text, truncation=True)\n        self.sums = tokenizer(self.sumry,padding='max_length',max_length=self.pad_sum, truncation=True)\n        \n        self.loss_on_prefix = loss_on_prefix\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        txts_tokens = self.txts[\"input_ids\"][idx]\n        txts_att=self.txts[\"attention_mask\"][idx]\n        sumry_tokens = self.sums[\"input_ids\"][idx]\n        sumry_tokens = [-100 if x==0 else x for x in sumry_tokens] \n        txts_tokens = th.tensor(txts_tokens)\n        txts_att = th.tensor(txts_att)\n        sumry_tokens=th.tensor(sumry_tokens)\n        \n        return dict(input_ids= txts_tokens, attention_mask=txts_att,labels=sumry_tokens)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_d=GSMDataset(tokenizer,train_dataset,max_txt_pad,max_smry_pad)\neval_d=GSMDataset(tokenizer,val_dataset,max_txt_pad,max_smry_pad)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING ARGUMENTS INITILIZATION AND TRAINING","metadata":{}},{"cell_type":"code","source":"output_dir = '/content/sample_data'\n\ntraining_args = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=12,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    eval_accumulation_steps=1, # Number of eval steps to keep in GPU (the higher, the mor vRAM used)\n    prediction_loss_only=True, # If I need co compute only loss and not other metrics, setting this to true will use less RAM\n    learning_rate=0.001,\n    evaluation_strategy='steps', # Run evaluation every eval_steps\n    save_steps=10000, # How often to save a checkpoint\n    save_total_limit=1, # Number of maximum checkpoints to save\n    remove_unused_columns=True, # Removes useless columns from the dataset\n    run_name='run_name', # Wandb run name\n    logging_steps=1000, # How often to log loss to wandb\n    eval_steps=1000, # How often to run evaluation on the val_set\n    logging_first_step=False, # Whether to log also the very first training step to wandb\n    load_best_model_at_end=True, # Whether to load the best model found at each evaluation.\n    metric_for_best_model=\"loss\", # Use loss to evaluate best model.\n    greater_is_better=False # Best model is the one with the lowest loss, not highest.\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_d,\n    eval_dataset=eval_d\n)\n\ntrainer.train()\ntrainer.save_model(output_dir + '/model')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PREDICTING","metadata":{}},{"cell_type":"code","source":"model = T5ForConditionalGeneration.from_pretrained('/content')\ntokenized_text = tokenizer(str(text),padding='max_length', max_length=127, truncation=True)\nsource_ids = th.tensor([tokenized_text['input_ids']])\nsource_mask = th.tensor([tokenized_text['attention_mask']])\ngenerated_ids = model.generate(\n        input_ids = source_ids,\n        attention_mask = source_mask, \n        max_length=127,\n        num_beams=5,\n        repetition_penalty=1, \n        length_penalty=1, \n        early_stopping=True,\n        no_repeat_ngram_size=2\n    )\npred = tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}